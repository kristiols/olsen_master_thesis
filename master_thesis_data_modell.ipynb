{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRAaFvBvdF2Q"
      },
      "outputs": [],
      "source": [
        "__author__ = \"Kristian Olsen / NMBU\"\n",
        "__email__ = \"kristian.olsen@nmbu.no\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2Sj5LFMUiIB"
      },
      "source": [
        "# Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm3tfsHtUgny"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, make_scorer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6LaWQCCUpT-"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QFpL1TCAHeb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load data from csv, the xlsx is beforehand converted into .csv for easier management\n",
        "df = pd.read_csv('/content/Cube-data_Tberg_total_24032021.csv', sep = ';')\n",
        "\n",
        "# show first rows and information to get initial understanding of the data\n",
        "print(df.head(), df.info())\n",
        "print(\"Shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmgwcL5EU101"
      },
      "source": [
        "# Data processing and feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_uJ3VQTU1tQ"
      },
      "outputs": [],
      "source": [
        "# rename features from Norwegian to English\n",
        "rename_dict= {'fra-sone':'origin-zone',\n",
        "              'til-sone':'destination-zone',\n",
        "              'fra-Grk':'origin-area',\n",
        "              'Til-Grk':'destination-area',\n",
        "              'Reiser':'Trips',\n",
        "              'TotBef':'TotPop',\n",
        "              'Arb':'Work',\n",
        "              'Handel':'Retail',\n",
        "              'FriHenPriv':'RecDelPriv',\n",
        "              'Gange':'Walk',\n",
        "              'Sykkel':'Bicycle',\n",
        "              'Bil':'Car',\n",
        "              'Bilpass':'Carpass'\n",
        "              }\n",
        "\n",
        "df = df.rename(columns = rename_dict)\n",
        "\n",
        "# drop columns that origin from travel survey and being duplicate features\n",
        "df = df.drop(['origin-area', # duplicate information\n",
        "              'destination-area', # duplicate information\n",
        "              'Ndays', # irrelevant\n",
        "              'Walk', # origins from travel survey\n",
        "              'Bicycle', # origins from travel survey\n",
        "              'Car', # origins from travel survey\n",
        "              'Carpass', # origins from travel survey\n",
        "              'PT'], # origins from travel survey\n",
        "              axis = 1\n",
        "             )\n",
        "\n",
        "# convert features from data object to numerical\n",
        "object_columns = ['Trips', 'Distance', 'TotPop']\n",
        "\n",
        "for column in object_columns:\n",
        "  df[column] = df[column].str.replace(',', '.').astype(float)\n",
        "  df[column] = pd.to_numeric(df[column])\n",
        "\n",
        "print(df.head()) # control the renaming\n",
        "\n",
        "# Check for missing values and duplicates\n",
        "print(df.isnull().sum())\n",
        "print(np.isnan(df).sum())\n",
        "print(\"Has duplicates:\", df.duplicated().any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPYS0RxZAXud"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix for feature engineering\n",
        "df_corr = df.drop('destination-zone', axis=1)\n",
        "corr_matrix = df_corr.corr()\n",
        "\n",
        "mask = np.zeros_like(corr_matrix, dtype=bool)\n",
        "mask[np.triu_indices_from(mask, k=1)] = True\n",
        "\n",
        "# Create a Correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdf3zPGVXVe4"
      },
      "source": [
        "## Statistical analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVflnJnOXTr9"
      },
      "outputs": [],
      "source": [
        "# simple statistical overview of all features\n",
        "df.describe()\n",
        "\n",
        "# Further statistical analysis was conducted directly in the excel file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rmpCANHVndf"
      },
      "source": [
        "## Principal Component Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNZl84T6AlZw"
      },
      "outputs": [],
      "source": [
        "# Initializing Loading-plot for whole dataset\n",
        "df_pca = df.drop('destination-zone', axis=1)\n",
        "scaler = StandardScaler()\n",
        "data_standardized = scaler.fit_transform(df_pca.T)\n",
        "\n",
        "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
        "principal_components = pca.fit_transform(data_standardized)\n",
        "\n",
        "feature_names = ['Origin-zone', 'Trips', 'Distance','TotPop','Work','Retail','RecDelPriv']\n",
        "\n",
        "# Plotting the PCA\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, (x, y) in enumerate(principal_components):\n",
        "    plt.scatter(x, y, marker='o', label=f'{feature_names[i]}')  # Plot points\n",
        "    plt.text(x, y, feature_names[i], fontsize=12)\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Plot')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(f\"Explained variance by component: {pca.explained_variance_ratio_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLOtIJZrAtVI"
      },
      "outputs": [],
      "source": [
        "# Initialize score plot of n = 10 000 samples from the dataset\n",
        "df_pca_2 = df\n",
        "df_pca_sample = df_pca_2.sample(n=10000, random_state=42)\n",
        "unique_labels_3 = df_pca_sample['destination-zone'].tolist()\n",
        "df_pca_sample = df_pca_sample.drop('destination-zone', axis=1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_standardized = scaler.fit_transform(df_pca_sample)\n",
        "\n",
        "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
        "principal_components = pca.fit_transform(data_standardized)\n",
        "\n",
        "# Plotting the PCA\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, (x, y) in enumerate(principal_components):\n",
        "    plt.scatter(x, y, marker=None,label=f'{unique_labels_3[i]}')\n",
        "    plt.text(x, y, unique_labels_3[i], fontsize=10)\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Plot')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(f\"Explained variance by component: {pca.explained_variance_ratio_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-rolQmZAwcA"
      },
      "outputs": [],
      "source": [
        "# Initialize loading plot of n = 10 000 samples from the dataset\n",
        "df_pca_3 = df\n",
        "df_pca_sample_2 = df_pca_3.sample(n=10000, random_state=42)\n",
        "data_standardized = scaler.fit_transform(df_pca_sample.T)\n",
        "\n",
        "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
        "principal_components = pca.fit_transform(data_standardized)\n",
        "\n",
        "feature_names = ['Origin-zone', 'Trips', 'Distance','TotPop','Work','Retail','RecDelPriv']\n",
        "\n",
        "# Plotting the PCA\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, (x, y) in enumerate(principal_components):\n",
        "    plt.scatter(x, y, marker='o', label=f'{feature_names[i]}')  # Plot points\n",
        "    plt.text(x * 1.05, y * 1.05, feature_names[i], fontsize=12)\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Plot')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(f\"Explained variance by component: {pca.explained_variance_ratio_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIROTea6AzRC"
      },
      "outputs": [],
      "source": [
        "# Intialize score plot for unique destination zones\n",
        "unique_df = df.drop_duplicates(subset='destination-zone', keep='first')\n",
        "df_pca_unique = unique_df.drop('destination-zone', axis=1)\n",
        "scaler = StandardScaler()\n",
        "data_standardized = scaler.fit_transform(df_pca_unique)\n",
        "\n",
        "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
        "principal_components = pca.fit_transform(data_standardized)\n",
        "\n",
        "unique_labels = df['destination-zone'].unique().tolist()\n",
        "\n",
        "# Plotting the PCA\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, (x, y) in enumerate(principal_components):\n",
        "    plt.scatter(x, y, marker=None,label=f'{unique_labels[i]}')  # Plot points\n",
        "    plt.text(x, y, unique_labels[i], fontsize=10)\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Plot')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(f\"Explained variance by component: {pca.explained_variance_ratio_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BXLxSicA1bA"
      },
      "outputs": [],
      "source": [
        "# Intialize loading plot for unique destination zones\n",
        "unique_df = df.drop_duplicates(subset='destination-zone', keep='first')\n",
        "df_pca_unique = unique_df.drop('destination-zone', axis=1)\n",
        "scaler = StandardScaler()\n",
        "data_standardized = scaler.fit_transform(df_pca_unique.T)\n",
        "\n",
        "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
        "principal_components = pca.fit_transform(data_standardized)\n",
        "\n",
        "feature_names = ['Origin-zone', 'Trips', 'Distance','TotPop','Work','Retail','RecDelPriv']\n",
        "\n",
        "# Plotting the PCA\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, (x, y) in enumerate(principal_components):\n",
        "    plt.scatter(x, y, marker='o', label=f'{feature_names[i]}')  # Plot points\n",
        "    plt.text(x * 1.05, y * 1.05, feature_names[i], fontsize=12)\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Plot')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(f\"Explained variance by component: {pca.explained_variance_ratio_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1veZ9QdXiLr"
      },
      "source": [
        "# Remove outliers and violin plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JlJjUD8A-0h"
      },
      "outputs": [],
      "source": [
        "# Violin plots of data\n",
        "data_to_plot = df.iloc[:, 2:8]\n",
        "\n",
        "# Set up a matplotlib figure and array of axes, with 1 row and enough columns\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(data_to_plot.columns), figsize=(15, 5))  # Adjust figsize as needed\n",
        "\n",
        "# Loop through each subplot axis and column data\n",
        "for ax, (colname, coldata) in zip(axes, data_to_plot.items()):\n",
        "    sns.violinplot(y=coldata, ax=ax)  # Use 'y' to specify the data\n",
        "    ax.set_title(colname)  # Set title to column name\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12OahhWzBCr5"
      },
      "outputs": [],
      "source": [
        "def remove_outliers(df, feature):\n",
        "    Q1 = df[feature].quantile(0.25)\n",
        "    Q3 = df[feature].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Filter out outliers\n",
        "    df_filtered = df[(df[feature] <= upper_bound)]\n",
        "    return df_filtered\n",
        "\n",
        "feature_list = ['Trips','Distance','Work','Retail','RecDelPriv']\n",
        "\n",
        "# Applying the function to each of the features\n",
        "features = feature_list\n",
        "for feature in features:\n",
        "    df = remove_outliers(df, feature)\n",
        "\n",
        "# Second visualization of violin plots to further investigate skewedness\n",
        "data_to_plot = df.iloc[:, 2:8]\n",
        "\n",
        "# Set up a figure\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(data_to_plot.columns), figsize=(15, 5))\n",
        "\n",
        "# Go through each subplot and column data\n",
        "for ax, (colname, coldata) in zip(axes, data_to_plot.items()):\n",
        "    sns.violinplot(y=coldata, ax=ax)  # Use 'y' to specify the data\n",
        "    ax.set_title(colname)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Occ_gbq_AfkG"
      },
      "outputs": [],
      "source": [
        "def remove_outliers(df, feature):\n",
        "    Q1 = df[feature].quantile(0.25)\n",
        "    Q3 = df[feature].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    upper_bound = Q3 + 0.5 * IQR\n",
        "\n",
        "    # Filter out outliers\n",
        "    df_filtered = df[(df[feature] <= upper_bound)]\n",
        "    return df_filtered\n",
        "\n",
        "feature_list = ['Trips','Work','Retail','RecDelPriv']\n",
        "\n",
        "# Applying the function\n",
        "features = feature_list\n",
        "for feature in features:\n",
        "    df = remove_outliers(df, feature)\n",
        "\n",
        "# A third visualization with violin plots\n",
        "data_to_plot = df.iloc[:, 2:8]\n",
        "\n",
        "# Set up a figure\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(data_to_plot.columns), figsize=(15, 5))  # Adjust figsize as needed\n",
        "\n",
        "# Go through each subplot axis and column data\n",
        "for ax, (colname, coldata) in zip(axes, data_to_plot.items()):\n",
        "    sns.violinplot(y=coldata, ax=ax)\n",
        "    ax.set_title(colname)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYADboJAYQHI"
      },
      "source": [
        "# Data modelling and hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7_ru0B2BWEp"
      },
      "outputs": [],
      "source": [
        "# Pipeline for Logistic Regression\n",
        "pipe_lr = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "param_grid_lr = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'classifier__solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "# Pipeline for SVC\n",
        "pipe_svc = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', SVC())\n",
        "])\n",
        "param_grid_svc = {\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "# Pipeline for Random Forest\n",
        "pipe_rf = Pipeline([\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "param_grid_rf = {\n",
        "    'classifier__n_estimators': [10, 50, 100, 200],\n",
        "    'classifier__max_depth': [None, 10, 20, 30]\n",
        "}\n",
        "\n",
        "# Pipeline for Gaussian Naive Bayes\n",
        "pipe_nb = Pipeline([\n",
        "    ('classifier', GaussianNB())\n",
        "])\n",
        "\n",
        "param_grid_nb = {}\n",
        "\n",
        "# seperate training data from response variable\n",
        "X = df.drop('destination-zone', axis=1)\n",
        "y = df['destination-zone']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def perform_grid_search(pipe, param_grid, X_train, y_train, X_test, y_test):\n",
        "    grid_search = GridSearchCV(pipe, param_grid, cv=2, verbose=1, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(\"Best parameters:\", grid_search.best_params_)\n",
        "    print(\"Best cross-validation score: {:.3f}\".format(grid_search.best_score_)) #shows accuracy performance\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    y_pred = grid_search.predict(X_test)\n",
        "\n",
        "    # Calculate Metrics\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    recall = recall_score(y_test, y_pred, average='macro')\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Apply the function to each classifier\n",
        "print(\"Logistic Regression Results:\")\n",
        "perform_grid_search(pipe_lr, param_grid_lr, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"SVC Results:\")\n",
        "perform_grid_search(pipe_svc, param_grid_svc, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"Random Forest Results:\")\n",
        "perform_grid_search(pipe_rf, param_grid_rf, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print('Gaussian Naive Bayes Results:')\n",
        "perform_grid_search(pipe_nb, param_grid_nb, X_train, y_train, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AZWZrXJZBBV"
      },
      "source": [
        "# Evaluation: Five-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nEVWeDeBeVx"
      },
      "outputs": [],
      "source": [
        "# Dictionaries with best hyperparameters\n",
        "lr_best_params = {\n",
        "    'classifier__C': [100],\n",
        "    'classifier__solver': ['lbfgs']\n",
        "}\n",
        "svc_best_params = {\n",
        "    'classifier__C': [10],\n",
        "    'classifier__kernel': ['linear']\n",
        "}\n",
        "rf_best_params = {\n",
        "    'classifier__n_estimators': [200],\n",
        "    'classifier__max_depth': [30]\n",
        "}\n",
        "\n",
        "# Model Selection with best parameters\n",
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(C=100, max_iter=1000, solver='lbfgs' ),\n",
        "    'SVC': SVC(C=10, kernel='linear'),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=200, max_depth=30),\n",
        "    'Naive Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "# Training and Validation\n",
        "results = {}\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'mcc': make_scorer(matthews_corrcoef),\n",
        "    'precision': make_scorer(precision_score, average='macro'),\n",
        "    'recall': make_scorer(recall_score, average='macro'),\n",
        "    'f1_score': make_scorer(f1_score, average='macro'),\n",
        "}\n",
        "for name, model in models.items():\n",
        "    # Create a pipeline with preprocessing and the model\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),  # Feature scaling\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_results = cross_validate(pipeline, X_train, y_train, cv=5, scoring=scoring, return_train_score=False)\n",
        "\n",
        "    # Training\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Testing\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Evaluation metrics on the test data\n",
        "    test_scores = {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred, average='macro'),\n",
        "        'Recall': recall_score(y_test, y_pred, average='macro'),\n",
        "        'F1-score': f1_score(y_test, y_pred, average='macro'),\n",
        "        'MCC': matthews_corrcoef(y_test, y_pred),\n",
        "\n",
        "    }\n",
        "\n",
        "    # Combine Cross validation  mean and standard deviation in results\n",
        "    results[name] = {\n",
        "        'CV Accuracy (mean)': np.mean(cv_results['test_accuracy']),\n",
        "        'CV Accuracy (std)': np.std(cv_results['test_accuracy']),\n",
        "        'CV Precision (mean)': np.mean(cv_results['test_precision']),\n",
        "        'CV Precision (std)': np.std(cv_results['test_precision']),\n",
        "        'CV Recall (mean)': np.mean(cv_results['test_recall']),\n",
        "        'CV Recall (std)': np.std(cv_results['test_recall']),\n",
        "        'CV F1-score (mean)': np.mean(cv_results['test_f1_score']),\n",
        "        'CV F1-score (std)': np.std(cv_results['test_f1_score']),\n",
        "        'CV MCC (mean)': np.mean(cv_results['test_mcc']),\n",
        "        'CV MCC (std)': np.std(cv_results['test_mcc']),\n",
        "        **test_scores\n",
        "    }\n",
        "\n",
        "    # Analyze feature importance\n",
        "    if name == 'LogisticRegression':\n",
        "      feature_importance_lr = pd.Series(model.coef_[0], index=X_train.columns)\n",
        "      feature_importance_lr_sorted = feature_importance_lr.abs().sort_values(ascending=False)\n",
        "\n",
        "      print(\"Logistic Regression Feature Importance:\")\n",
        "      print(feature_importance_lr_sorted)\n",
        "\n",
        "    elif name == 'SVC':\n",
        "        feature_importance_svc = pd.Series(model.coef_[0], index=X_train.columns)\n",
        "        feature_importance_svc_sorted = feature_importance_svc.abs().sort_values(ascending=False)\n",
        "\n",
        "        print(\"SVC Feature Importance:\")\n",
        "        print(feature_importance_svc_sorted)\n",
        "    elif name == 'RandomForest':\n",
        "        # Get feature importances\n",
        "        importances = model.feature_importances_\n",
        "\n",
        "        # Sort feature importances in descending order\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "\n",
        "        # Print feature ranking\n",
        "        print(\"Feature ranking:\")\n",
        "        print(model)\n",
        "        print(\"Feature ranking:\")\n",
        "        for f in range(X_train.shape[1]):\n",
        "          print(f\"{f + 1}. Feature {indices[f]} ({X_train.columns[indices[f]]}): {importances[indices[f]]}\")\n",
        "\n",
        "    elif name == 'Naive Bayes':\n",
        "        feature_variance = X_train.groupby(y).var()\n",
        "\n",
        "        print(\"GaussianNB Feature Variance:\")\n",
        "        print(feature_variance)\n",
        "    else:\n",
        "      pass\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}